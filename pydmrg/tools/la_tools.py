#!/usr/bin/env python
#
# Author: Qiming Sun <osirpt.sun@gmail.com>
#
# Adapted by Phillip Helms

'''
Extension to scipy.linalg module
'''

import warnings
from cyclomps.tools.utils import *
import numpy
import copy
import scipy.linalg

def pick_real_eigs(w, v, nroots, x0):
    # Here we pick the eigenvalues with smallest imaginary component,
    # where we are forced to choose at least one eigenvalue.
    w = to_nparray(w)
    abs_imag = abs(w.imag)
    max_imag_tol = max(1e-10,min(abs_imag)*1.1)
    realidx = numpy.where((abs_imag < max_imag_tol))[0]
    if len(realidx) < nroots:
        idx = w.real.argsort()
    else:
        idx = realidx[w[realidx].real.argsort()]
    w = from_nparray(w)
    w = real(take(w,idx))
    v = real(take(v,idx,axis=1))
    return w, v, idx

def davidson(aop, x0, precond, tol=1e-12, max_cycle=50, max_space=12,
             lindep=1e-14, max_memory=2000, callback=None,
             nroots=1, lessio=False, follow_state=False):
    '''Davidson diagonalization method to solve  a c = e c.  Ref
    [1] E.R. Davidson, J. Comput. Phys. 17 (1), 87-94 (1975).
    [2] http://people.inf.ethz.ch/arbenz/ewp/Lnotes/chapter11.pdf

    Args:
        aop : function(x) => array_like_x
            aop(x) to mimic the matrix vector multiplication :math:`\sum_{j}a_{ij}*x_j`.
            The argument is a 1D array.  The returned value is a 1D array.
        x0 : 1D array or a list of 1D array
            Initial guess.  The initial guess vector(s) are just used as the
            initial subspace bases.  If the subspace is smaller than "nroots",
            eg 10 roots and one initial guess, all eigenvectors are chosen as
            the eigenvectors during the iterations.  The first iteration has
            one eigenvector, the next iteration has two, the third iterastion
            has 4, ..., until the subspace size > nroots.
        precond : function(dx, e, x0) => array_like_dx
            Preconditioner to generate new trial vector.
            The argument dx is a residual vector ``a*x0-e*x0``; e is the current
            eigenvalue; x0 is the current eigenvector.

    Kwargs:
        tol : float
            Convergence tolerance.
        max_space : int
            space size to hold trial vectors.
        lindep : float
            Linear dependency threshold.  The function is terminated when the
            smallest eigenvalue of the metric of the trial vectors is lower
            than this threshold.
        max_memory : int or float
            Allowed memory in MB.
        callback : function(envs_dict) => None
            callback function takes one dict as the argument which is
            generated by the builtin function :func:`locals`, so that the
            callback function can access all local variables in the current
            envrionment.
        nroots : int
            Number of eigenvalues to be computed.  When nroots > 1, it affects
            the shape of the return value
        lessio : bool
            How to compute a*x0 for current eigenvector x0.  There are two
            ways to compute a*x0.  One is to assemble the existed a*x.  The
            other is to call aop(x0).  The default is the first method which
            needs more IO and less computational cost.  When IO is slow, the
            second method can be considered.
        follow_state : bool
            If the solution dramatically changes in two iterations, clean the
            subspace and restart the iteration with the old solution.  It can
            help to improve numerical stability.  Default is False.

    Returns:
        e : float or list of floats
            Eigenvalue.  By default it's one float number.  If :attr:`nroots` > 1, it
            is a list of floats for the lowest :attr:`nroots` eigenvalues.
        c : 1D array or list of 1D arrays
            Eigenvector.  By default it's a 1D array.  If :attr:`nroots` > 1, it
            is a list of arrays for the lowest :attr:`nroots` eigenvectors.
    '''
    e, x = davidson1(lambda xs: [aop(x) for x in xs],
                     x0, precond, tol, max_cycle, max_space, lindep,
                     max_memory, callback, nroots, lessio,
                     follow_state)[1:]
    if nroots == 1:
        return e[0], x[0]
    else:
        return e, x

def davidson1(aop, x0, precond, tol=1e-12, max_cycle=50, max_space=12,
             lindep=1e-14, max_memory=2000, callback=None,
             nroots=1, lessio=False, follow_state=False):
    '''Davidson diagonalization method to solve  a c = e c.  Ref
    [1] E.R. Davidson, J. Comput. Phys. 17 (1), 87-94 (1975).
    [2] http://people.inf.ethz.ch/arbenz/ewp/Lnotes/chapter11.pdf

    Args:
        aop : function([x]) => [array_like_x]
            Matrix vector multiplication :math:`y_{ki} = \sum_{j}a_{ij}*x_{jk}`.
        x0 : 1D array or a list of 1D array
            Initial guess.  The initial guess vector(s) are just used as the
            initial subspace bases.  If the subspace is smaller than "nroots",
            eg 10 roots and one initial guess, all eigenvectors are chosen as
            the eigenvectors during the iterations.  The first iteration has
            one eigenvector, the next iteration has two, the third iterastion
            has 4, ..., until the subspace size > nroots.
        precond : function(dx, e, x0) => array_like_dx
            Preconditioner to generate new trial vector.
            The argument dx is a residual vector ``a*x0-e*x0``; e is the current
            eigenvalue; x0 is the current eigenvector.

    Kwargs:
        tol : float
            Convergence tolerance.
        max_cycle : int
            max number of iterations.
        max_space : int
            space size to hold trial vectors.
        lindep : float
            Linear dependency threshold.  The function is terminated when the
            smallest eigenvalue of the metric of the trial vectors is lower
            than this threshold.
        max_memory : int or float
            Allowed memory in MB.
        callback : function(envs_dict) => None
            callback function takes one dict as the argument which is
            generated by the builtin function :func:`locals`, so that the
            callback function can access all local variables in the current
            envrionment.
        nroots : int
            Number of eigenvalues to be computed.  When nroots > 1, it affects
            the shape of the return value
        lessio : bool
            How to compute a*x0 for current eigenvector x0.  There are two
            ways to compute a*x0.  One is to assemble the existed a*x.  The
            other is to call aop(x0).  The default is the first method which
            needs more IO and less computational cost.  When IO is slow, the
            second method can be considered.
        follow_state : bool
            If the solution dramatically changes in two iterations, clean the
            subspace and restart the iteration with the old solution.  It can
            help to improve numerical stability.  Default is False.

    Returns:
        conv : bool
            Converged or not
        e : list of floats
            The lowest :attr:`nroots` eigenvalues.
        c : list of 1D arrays
            The lowest :attr:`nroots` eigenvectors.
    '''

    toloose = sqrt(tol)

    if not isinstance(x0,list):
        x0 = [x0]

    max_space = max_space + nroots * 3

    heff = None
    fresh_start = True
    e = 0
    v = None
    conv = [False] * nroots
    emin = None

    for icyc in range(max_cycle):
        if fresh_start:
            xs = []
            ax = []
            space = 0
            xt = None
            xt, x0 = orth(x0), None
            max_dx_last = 1e9
        elif len(xt) > 1:
            xt = orth(xt)
            xt = xt[:40]  # 40 trial vectors at most

        axt = aop(xt)
        for k in range(len(xt)):
            xi = xt[k]
            xs.append(xt[k])
            ax.append(axt[k])
        rnow = len(xt)
        head, space = space, space+rnow

        if heff is None:  # Lazy initilize heff to determine the dtype
            heff = zeros((max_space+nroots,max_space+nroots), dtype=axt[0].dtype)

        elast = e
        vlast = v
        conv_last = conv
        for i in range(space):
            if head <= i < head+rnow:
                for k in range(i-head+1):
                    heff[head+k,i] = summ(dot(xt[k],axt[i-head]))
                    heff[i,head+k] = heff[head+k,i]
            else:
                for k in range(rnow):
                    heff[head+k,i] = summ(dot(xt[k],ax[i]))
                    heff[i,head+k] = heff[head+k,i]

        if USE_CTF:
            heff = to_nparray(heff)
        heff = real(heff).astype(axt[0].dtype)
        w, v = scipy.linalg.eigh(heff[:space,:space])
        if USE_CTF:
            w = from_nparray(w)
            v = from_nparray(v)
        e = w[:nroots]
        v = v[:,:nroots]

        x0 = _gen_x0(v, xs)
        ax0 = _gen_x0(v, ax)

        elast, conv_last = _sort_elast(elast, conv_last, vlast, v, fresh_start)
        de = e-elast
        dx_norm = []
        xt = []
        conv = [False] * nroots
        for k in range(e.shape[0]):
            ek = e[k]
            xt.append(ax0[k] - ek * x0[k])
            dx_norm.append(sqrt(real(dot(xt[k], xt[k]))))
            if not (abs(de[k]) < tol) & (dx_norm[k] < toloose):
                conv[k] = False

        ax0 = None
        max_dx_norm = max(dx_norm)
        ide = numpy.argmax(abs(de))
        if all(conv):
            break
        elif (follow_state and max_dx_norm > 1 and
              max_dx_norm/max_dx_last > 3 and space > nroots*3):
            x0 = _gen_x0(vlast, xs)
            fresh_start = True
            continue


        # remove subspace linear dependency
        conv_lindep = False
        for k in range(len(dx_norm)):
            if (not conv[k]) and (dx_norm[k]**2. > lindep): conv_lindep = True
        if conv_lindep:
            for k in range(e.shape[0]):
                ek = e[k]
                if (not conv[k]) and dx_norm[k]**2 > lindep:
                    xt[k] = precond(xt[k], e[0], x0[k])
                    xt[k] *= 1./sqrt(real(dot(xt[k], xt[k])))
                else:
                    xt[k] = None
        else:
            for k in range(e.shape[0]):
                ek = e[k]
                if dx_norm[k]**2 > lindep:
                    xt[k] = precond(xt[k], e[0], x0[k])
                    xt[k] *= 1./sqrt(real(dot(xt[k], xt[k])))
                else:
                    xt[k] = None
        xt = [xi for xi in xt if xi is not None]

        for i in range(space):
            xsi = xs[i]
            for xi in xt:
                xi -= xsi * dot(xsi, xi)
        norm_min = 1
        for i in range(len(xt)):
            xi = xt[i]
            norm = sqrt(real(dot(xi, xi)))
            if norm**2 > lindep:
                xt[i] *= 1./norm
                norm_min = min(norm_min, norm)
            else:
                xt[i] = None
        xt = [xi for xi in xt if xi is not None]
        xi = None
        if len(xt) == 0:
            conv = [conv[k] or (norm < toloose) for k,norm in enumerate(dx_norm)]
            break

        max_dx_last = max_dx_norm
        fresh_start = space+nroots > max_space

        if callable(callback):
            callback(locals())

    return conv, e, x0


def eigh(a, *args, **kwargs):
    nroots = kwargs.get('nroots', 1)
    if isinstance(a, numpy.ndarray) and a.ndim == 2:
        e, v = scipy.linalg.eigh(a)
        if nroots == 1:
            return e[0], v[:,0]
        else:
            return e[:nroots], v[:,:nroots].T
    else:
        return davidson(a, *args, **kwargs)
dsyev = eigh

def eig(aop, x0, precond, tol=1e-12, max_cycle=50, max_space=12,
        lindep=1e-14, max_memory=2000, callback=None,
        nroots=1, lessio=False, left=False, pick=pick_real_eigs,
        follow_state=False):
    '''Davidson diagonalization to solve the non-symmetric eigenvalue problem

    Args:
        aop : function([x]) => [array_like_x]
            Matrix vector multiplication :math:`y_{ki} = \sum_{j}a_{ij}*x_{jk}`.
        x0 : 1D array or a list of 1D array
            Initial guess.  The initial guess vector(s) are just used as the
            initial subspace bases.  If the subspace is smaller than "nroots",
            eg 10 roots and one initial guess, all eigenvectors are chosen as
            the eigenvectors during the iterations.  The first iteration has
            one eigenvector, the next iteration has two, the third iterastion
            has 4, ..., until the subspace size > nroots.
        precond : function(dx, e, x0) => array_like_dx
            Preconditioner to generate new trial vector.
            The argument dx is a residual vector ``a*x0-e*x0``; e is the current
            eigenvalue; x0 is the current eigenvector.

    Kwargs:
        tol : float
            Convergence tolerance.
        max_cycle : int
            max number of iterations.
        max_space : int
            space size to hold trial vectors.
        lindep : float
            Linear dependency threshold.  The function is terminated when the
            smallest eigenvalue of the metric of the trial vectors is lower
            than this threshold.
        max_memory : int or float
            Allowed memory in MB.
        callback : function(envs_dict) => None
            callback function takes one dict as the argument which is
            generated by the builtin function :func:`locals`, so that the
            callback function can access all local variables in the current
            envrionment.
        nroots : int
            Number of eigenvalues to be computed.  When nroots > 1, it affects
            the shape of the return value
        lessio : bool
            How to compute a*x0 for current eigenvector x0.  There are two
            ways to compute a*x0.  One is to assemble the existed a*x.  The
            other is to call aop(x0).  The default is the first method which
            needs more IO and less computational cost.  When IO is slow, the
            second method can be considered.
        left : bool
            Whether to calculate and return left eigenvectors.  Default is False.
        pick : function(w,v,nroots) => (e[idx], w[:,idx], idx)
            Function to filter eigenvalues and eigenvectors.
        follow_state : bool
            If the solution dramatically changes in two iterations, clean the
            subspace and restart the iteration with the old solution.  It can
            help to improve numerical stability.  Default is False.

    Returns:
        conv : bool
            Converged or not
        e : list of eigenvalues
            The eigenvalues can be sorted real or complex, depending on the
            return value of ``pick`` function.
        vl : list of 1D arrays
            Left eigenvectors. Only returned if ``left=True``.
        c : list of 1D arrays
            Right eigenvectors.

    '''
    res = davidson_nosym1(lambda xs: [aop(x) for x in xs],
                          x0, precond, tol, max_cycle, max_space, lindep,
                          max_memory, callback, nroots, lessio,
                          left, pick, follow_state)
    if left:
        e, vl, vr = res[1:]
        if nroots == 1:
            return e[0], vl[0], vr[0]
        else:
            return e, vl, vr
    else:
        e, x = res[1:]
        if nroots == 1:
            return e[0], x[0]
        else:
            return e, x
davidson_nosym = eig

def davidson_nosym1(aop, x0, precond, tol=1e-12, max_cycle=50, max_space=12,
                    lindep=1e-14, max_memory=2000, callback=None,
                    nroots=1, lessio=False, left=False, pick=pick_real_eigs,
                    follow_state=False):

    toloose = sqrt(tol)

    if not isinstance(x0,list):
        x0 = [x0]

    max_space = max_space + nroots * 3

    heff = None
    fresh_start = True
    e = 0
    v = None
    conv = [False] * nroots
    emin = None

    for icyc in range(max_cycle):
        if fresh_start:
            xs = []
            ax = []
            space = 0
            xt = None
            xt, x0 = orth(x0), None
            max_dx_last = 1e9
        elif len(xt) > 1:
            xt = orth(xt)
            xt = xt[:40]  # 40 trial vectors at most

        axt = aop(xt)
        for k in range(len(xt)):
            xi = xt[k]
            xs.append(xt[k])
            ax.append(axt[k])
        rnow = len(xt)
        head, space = space, space+rnow

        if heff is None:  # Lazy initilize heff to determine the dtype
            heff = zeros((max_space+nroots,max_space+nroots), dtype=axt[0].dtype)
        #else:
        #    heff = numpy.asarray(heff, dtype=axt[0].dtype)

        elast = e
        vlast = v
        conv_last = conv
        for i in range(rnow):
            for k in range(rnow):
                heff[head+k,head+i] = summ(dot(conj(xt[k]), axt[i]))
        for i in range(head):
            axi = ax[i]
            xi = xs[i]
            for k in range(rnow):
                heff[head+k,i] = summ(dot(conj(xt[k]), axi))
                heff[i,head+k] = summ(dot(conj(xi), axt[k]))

        if USE_CTF:
            heff = to_nparray(heff)
        w, v = scipy.linalg.eig(heff[:space,:space])
        if USE_CTF:
            w = from_nparray(w)
            v = from_nparray(v)
        e, v, idx = pick(w, v, nroots, locals())
        e = e[:nroots]
        v = v[:,:nroots]

        x0 = _gen_x0(v, xs)
        ax0 = _gen_x0(v, ax)

        elast, conv_last = _sort_elast(elast, conv_last, vlast, v, fresh_start)
        try:
            de = e-elast[:len(e)]
        except:
            de = e-elast
        dx_norm = []
        xt = []
        for k in range(e.shape[0]):
            ek = e[k]
            xt.append(ax0[k] - ek * x0[k])
            dx_norm.append(sqrt(real(dot(conj(xt[k]), xt[k]))))

        # Check for convergence
        conv = [False]*de.shape[0]
        for i in range(de.shape[0]):
            if not (abs(de[i]) < tol) & (dx_norm[i] < toloose):
                conv[i] = False
        #conv = (abs(de) < tol) & (dx_norm < toloose)

        ax0 = None
        max_dx_norm = max(dx_norm)
        ide = numpy.argmax(abs(de))
        if all(conv):
            break
        elif (follow_state and max_dx_norm > 1 and
              max_dx_norm/max_dx_last > 3 and space > nroots*3):
            x0 = _gen_x0(vlast, xs)
            fresh_start = True
            continue

        # remove subspace linear dependency
        conv_lindep = False
        for k in range(len(dx_norm)):
            if (not conv[k]) and (dx_norm[k]**2. > lindep): conv_lindep = True
        if conv_lindep:
            for k in range(e.shape[0]):
                ek = e[k]
                if (not conv[k]) and dx_norm[k]**2 > lindep:
                    xt[k] = precond(xt[k], e[0], x0[k])
                    xt[k] *= 1./sqrt(real(dot(conj(xt[k]), xt[k])))
                else:
                    xt[k] = None
        else:
            for k in range(e.shape[0]):
                ek = e[k]
                if dx_norm[k]**2 > lindep:
                    xt[k] = precond(xt[k], e[0], x0[k])
                    xt[k] *= 1./sqrt(real(dot(conj(xt[k]), xt[k])))
                else:
                    xt[k] = None
        xt = [xi for xi in xt if xi is not None]

        for i in range(space):
            xsi = xs[i]
            for xi in xt:
                xi -= xsi * dot(conj(xsi), xi)
        norm_min = 1
        for i in range(len(xt)):
            xi = xt[i]
            norm = sqrt(real(dot(conj(xi), xi)))
            if norm**2 > lindep:
                xt[i] *= 1./norm
                norm_min = min(norm_min, norm)
            else:
                xt[i] = None
        xt = [xi for xi in xt if xi is not None]
        xi = None
        if len(xt) == 0:
            conv = [conv[k] or (norm < toloose) for k,norm in enumerate(dx_norm)]
            break

        max_dx_last = max_dx_norm
        fresh_start = space+nroots > max_space

        if callable(callback):
            callback(locals())

    if left:
        warnings.warn('Left eigenvectors from subspace diagonalization method may not be converged')
        w, vl, v = scipy.linalg.eig(heff[:space,:space], left=True)
        e, v, idx = pick(w, v, nroots, x0)
        xl = _gen_x0(vl[:,idx[:nroots]].conj(), xs)
        x0 = _gen_x0(v[:,:nroots], xs)
        return conv, e[:nroots], xl, x0
    else:
        return conv, e, x0

"""
def orth(v):
    '''
    Orthogonalize vectors
    '''
    # Check if vectors are stored as list
    if isinstance(v,list):
        norm = sqrt(real(dot(conj(v[0]), v[0])))
        u = [v[0]/norm]
        for i in range(1, len(v)):
            vi = copy.deepcopy(v[i])+0.j
            for j in range(len(u)):
                vi -= (u[j]+0.j) * dot(conj(u[j]+0.j), vi)
            norm = sqrt(real(dot(conj(vi), vi)))
            u.append(vi/norm)
        return u
    else:
        vec_len,nVecs = v.shape
        norm = sqrt(real(dot(conj(v[:,0]),v[:,0])))
        v[:,0] /= norm
        for i in range(1,nVecs):
            for j in range(i):
                v[:,i] -= v[:,j] * dot(conj(v[:,j]),v[:,i])
            norm = sqrt(real(dot(conj(v[:,i]),v[:,i])))
            v[:,i] /= norm
        return v
"""
def orth(v):
    """
    Orthogonalize vectors
    """
    if isinstance(v,list):
        norm = sqrt(real(dot(conj(v[0]), v[0])))
        u = [v[0]/norm]
        for i in range(1, len(v)):
            vi = copy.deepcopy(v[i])+0.j
            for j in range(len(u)):
                vi -= (u[j]+0.j) * dot(conj(u[j]+0.j), vi)
            norm = sqrt(real(dot(conj(vi), vi)))
            u.append(vi/norm)
        return u
    else:
        q,_ = qr(v)
        return q

#def _gen_x0(v, xs):
#    space, nroots = v.shape
#    x0 = []
#    for k in range(nroots):
#        x0.append(xs[space-1] * v[space-1,k])
#    for i in reversed(range(space-1)):
#        xsi = xs[i]
#        for k in range(nroots):
#            x0[k] += v[i,k] * xsi
#    return x0

def _gen_x0(v,xs):
    space,nroots = v.shape
    x0 = []
    x0 = make_it(v,xs,x0,nroots,space)
    x0 = make_it2(space,xs,nroots,x0,v)
    return x0

def make_it(v,xs,x0,nroots,space):
    for k in range(nroots):
        x0.append(xs[space-1]*v[space-1,k])
    return x0

def make_it2(space,xs,nroots,x0,v):
    for i in reversed(range(space-1)):
        xsi = xs[i]
        for k in range(nroots):
            x0[k] += v[i,k] * xsi
    return x0

def _sort_elast(elast, conv_last, vlast, v, fresh_start):
    if fresh_start:
        return elast, conv_last
    head, nroots = vlast.shape
    ovlp = abs((dot(conj(transpose(v[:head])), vlast)))
    idx = numpy.argmax(to_nparray(ovlp), axis=1)
    return [elast[i] for i in idx], [conv_last[i] for i in idx]
